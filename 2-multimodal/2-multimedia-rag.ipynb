{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "1. OpenAI Python Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate, json\n",
    "\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "client.is_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal RAG\n",
    "\n",
    "> Retreive Image → Pass to LMM (Large Multimodal Model) → Get Text/Image Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 – Retrieve content from the database with a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import weaviate.classes.query as wq\n",
    "\n",
    "animals = client.collections.get(\"Animals\")\n",
    "\n",
    "def retrieve_image(query):\n",
    "    response = animals.query.near_text(\n",
    "        query=query,\n",
    "        filters=wq.Filter.by_property(\"mediaType\").equal(\"image\"),\n",
    "        return_properties=['name','path','mediaType','image'],\n",
    "        limit = 1,\n",
    "    )\n",
    "    result = response.objects[0].properties\n",
    "\n",
    "    print(\"Retrieved image object:\",json.dumps(result, indent=2))\n",
    "\n",
    "    return result\n",
    "\n",
    "# response = retrieve_image(\"animal on a log\")\n",
    "response = retrieve_image(\"dog with a sign\")\n",
    "\n",
    "SOURCE_IMAGE = response['image']\n",
    "\n",
    "Image(response['path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - generate a description of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import openai, os\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "def generate_description_from_image_gpt4(prompt, image64):\n",
    "  headers = {\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {openai.api_key}\"\n",
    "  }\n",
    "\n",
    "  payload = {\n",
    "      \"model\": \"gpt-4-vision-preview\",\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": prompt\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                # \"url\": f\"data:image/jpeg;base64,{response.objects[0].properties['image']}\" #base64 encoded image from Weaviate\n",
    "                \"url\": f\"data:image/jpeg;base64,{image64}\" #base64 encoded image from Weaviate\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"max_tokens\": 300\n",
    "  }\n",
    "\n",
    "  response_oai = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "  result = response_oai.json()['choices'][0]['message']['content']\n",
    "  print(f\"Generated description: {result}\")\n",
    "\n",
    "  return result\n",
    "\n",
    "\n",
    "GENERATED_DESCRIPTION = generate_description_from_image_gpt4(\n",
    "    prompt=\"This is an image of my pet, please give me a cute and vivid description.\",\n",
    "    image64=SOURCE_IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - use the image description to generate a new image with DALL·E 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def generate_image_dalee3(prompt):\n",
    "  openai_client = OpenAI()\n",
    "\n",
    "  response_oai = openai_client.images.generate(\n",
    "    model=\"dall-e-3\",\n",
    "    prompt=str(prompt),\n",
    "    size=\"1792x1024\",\n",
    "    quality=\"standard\",\n",
    "    n=1,\n",
    "  )\n",
    "\n",
    "  result = response_oai.data[0].url\n",
    "  print (f\"Generated image url: {result}\")\n",
    "\n",
    "  return result\n",
    "\n",
    "image_url = generate_image_dalee3(GENERATED_DESCRIPTION)\n",
    "Image(url=image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - retrieve an image – Weaviate\n",
    "retrieved_image = retrieve_image(\"animal on a log\")\n",
    "SOURCE_IMAGE = retrieved_image['image']\n",
    "\n",
    "# Step 2 - generate a description - GPT4\n",
    "GENERATED_DESCRIPTION = generate_description_from_image_gpt4(\n",
    "    prompt=\"This is an image of my pet, please give me a cute and vivid description.\",\n",
    "    image64=SOURCE_IMAGE\n",
    ")\n",
    "\n",
    "# Step 3 - use the description to generate a new image – DALE-E 3\n",
    "GENERATED_IMAGE_URL = generate_image_dalee3(GENERATED_DESCRIPTION)\n",
    "\n",
    "Image(url=str(GENERATED_IMAGE_URL))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
